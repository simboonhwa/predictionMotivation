---
title: "Machine Learning: Module 8"
output:
  html_document:
    keep_md: yes
---

###
##Title: "Human Activity Recognition"
###
## 1. 		Executive Summary	
###

Data set related to exercise activity obtained from: http://groupware.les.inf.puc-rio.br/har was explored, analyse and model was builded and identify in order to help to predict how well activities (A,B,C,D,E) were performed. "A" is activity perform based on standard specification. Wherease "B","C","D" and "E" are activities which are intentionally performed "badly".



###
## 2. Environment preparation
###	

```{r global_options, include=FALSE, echo=TRUE, message=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

This is optional code to ustilise the mulitcore capability
```{r,message=FALSE}
#optional: set multicore
library(cluster)
library(parallel)
library(doSNOW)

coreNumber=max(detectCores(),1)
cluster=makeCluster(coreNumber, type = "SOCK",outfile="")
registerDoSNOW(cluster)
```

Library and files are read in preparation of data exploratory analysis
```{r, message=FALSE}
library(caret)
library(dplyr)
trainRaw <- read.csv("./pml-training.csv")
testRaw <- read.csv("./pml-testing.csv")
```

###
## 2. Exploratorty Data Analysis
###	

Since based the outcome (classe), the values gathered through the devices are stored in the column with words "belt", "forearm", "arm", and "dumbell", therefore these predictors (together with classe) are extracted. 

```{r}
mystr = grep("belt|forearm|arm|dumbell|classe", colnames(trainRaw), value = TRUE)
trainRaw= trainRaw[,mystr]

#remove those columns with all value = NA since this will not add value to the prediction. ??? are identified with on NA value and removed.

# colnames are identified
coln=colnames(trainRaw [,colSums(is.na( trainRaw )) != 0])

# column index are identify to apply to the test set.
coli=match(coln,colnames(trainRaw))
trainClean = trainRaw [,-coli]

# remove those predictors with near Zero Variance since this predictors with thsee property does not contribute to prediction. 34 are identified and removed.
temp=nearZeroVar(trainClean)
trainClean=trainClean[,-temp]

#nzv=nearZeroVar(trainClean, saveMetrics= TRUE)
#nzv[nzv$nzv,]

#append column index for "near zero var" predicators into coli.
#coli=c(coli,temp) %>% unique %>% sort


#col_names <- names(trainRaw)
#lapply(trainRaw[,col_names] , factor)
#temp=lapply(trainRaw[,colnames(trainClean)] , is.factor)
#str(temp)
```

###
## 3. Dataset Slicing
###	

```{r}
# Training set & Testing set are preparated using slicing
set.seed(1234) 
trainSet = createDataPartition(y=trainClean$classe, p=0.7, list=F)
trainData = trainClean[trainSet,]
testData = trainClean[-trainSet,]
```


###
## 3. Fit & Strategy for Model Selection
## 	a. Non-linear modeling (Decision Tree)
###	

The outcome (classe) is observed to be categorise (non-linear), therefore non-linear regression modeling is recommended. Two models (Decision Tree and Random Forest) will be built.


```{r,message=TRUE}
#Decision tree (rpart). rpart function will bootstrap apply
modelCART=train(classe ~., data=trainData, method="rpart")
#modelCART$finalModel
predictCART=predict(modelCART, testData)
CART=confusionMatrix(testData$classe, predictCART)
#accurarcyCART=postResample(predictCART,testData)
```

###
## 	b. Non-linear modeling (Random Tree)
###	

The Random Tree model is robust to correlated covariates & outliners.

```{r,message=TRUE}
# 5 fold cross validation is apply
controlRF = trainControl(method="cv", 2)
modelRF = train(classe ~ ., data=trainData, method="rf", trControl=controlRF, ntree=250)
predictRF=predict(modelRF, testData)
RF=confusionMatrix(testData$classe, predictRF)
#accurarcyRF=postResample(predictRF,testData)
```

```{r}
# Randown forest with PCA --- option 
# resource intensive
# random forest with pca will apply center and scale by default
# error using 0.8 with muticore
#ctrl = trainControl(preProcOptions = list(thresh = 0.9))
#modelRFPCA = train(classe ~ ., data = trainData, preProcess="pca", trControl = ctrl, method="rf")
#predictRFPCA=predict(modelRFPCA, testData)
#RFPCA=confusionMatrix(testData$classe, predictRFPCA)
```
###
## 3. Conclusion for model
###	
```{r}
# Display the accurarcy
#accurarcyCART
#accurarcyRF
# inspect the predictors that was selected.
varImp(modelCART)
varImp(modelRF)
```
